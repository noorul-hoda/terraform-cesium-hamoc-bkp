## terraform-devops-rnd

The AWS services used are classified into different modules as follows.

#### Modules

Needs

 - backend
     - API Gateway
     - ECR
     - Lambda
     - API GW Logging
 - cicd-mgmnt
     - SNS
     - Codestar connection
     - Cloudwatch event
     - Cloudwatch Log groups
 - lambda_s3-deploy
     - Codepipeline
     - Codebuild
     - IAM Roles
 - ecs-deploy
     - Codepipeline
     - Codebuild
     - IAM Roles
 - ecs
     - ACM
     - R53
     - ECR
     - ECS
     - ALB
     - Taskdefinition
     - IAM Roles
     - WAFV2
 - lambda_edge
     - lambda_edge
     - IAM Roles
 - frontend-core
     - Cloudfront
     - Cognito
     - S3
     - R53
     - ACM
 - wafv2
 - cloudwatch
     - SNS 
     - Cloudtrail
     - IAM roles
     - KMS
     - Cloudwatch Alarms
 - networking
     - VPC
     - Bastion EC2
     - ETL EC2
     - Security Groups
     - SSH Keys
     - IAM Role
 - Storage
     - RDS
     - S3

### Prerequisites
 
 **Note** The steps below is a rough note. For detailed steps please refer the provided documentation
 
- 1. Create R53 Hosted Zones as per the following information.
- 2. Create DynamoDB tables and S3 Buckets for storing terraform state with state locking
- 3. Create an IAM user in each account with admin privileges (Programmatic access only)
- 4.  Configure IAM profiles in local machine for each environment using AWS Access/Secret keys obtained earlier
- 5. Create a sample ECR in each aws account which is needed for lambda resource creation using images.
- 6. Pull the latest terraform code and edit the terraform.tf file and add the new backend block.
- 7. Create the SSH keys for each environment using ssh-keygen
- 8. Make the necessary changes in variable files as per the requirement of each environment
- 9. Comment the backend block wrt AWS Account and execute the init reconfigure command and create workspaces in each backends. (dev, qa, prod)
- 10. Verify the workspaces are correct in each backends. (Backend dev have dev workspace only, backend prod have prod workspace and backend qa have qa workspace only)

### Commands

#### Switching to different workspace which are in different AWS Accounts

Modify the S3 Backend block in terraform.tf which resides in the root folder.

ie, Comment the backend block wrt AWS Account and execute the init reconfigure command provided below (***Also make sure the tfvars have the correct AWS Profile set that is similiar to the prfile specified in backend/block***)

```
terraform init --reconfigure
```

#### Plan

##### The **terraform plan --var-file=./variables/<workspace>.tfvars** command is used to create an execution plan for different workspaces.

Variables are stored in ./variables/ folder in root directory (<workspace>.tfvars)

* To list and switch to workspace

```
terraform workspace list
terraform workspace select <workspace-name>
```



* To execute plan command for all the modules.

```
terraform plan --var-file=./variables/<workspace_name>.tfvars
```

* To execute plan command for a particular module.

```
terraform plan --target=module.(module_name) --var-file=./variables/<workspace_name>.tfvars
```

eg:- terraform plan --target=module.ecs --var-file=./variables/dev.tfvars


#### Apply

##### The **terraform apply --var-file=./variables/<workspace>.tfvars** command is used to apply the changes required to reach the desired state of the configuration, or the pre-determined set of actions generated by a terraform plan execution plan. Make sure to select to appropriate workspace before applying.

* To execute apply command for all the modules.

```
terraform apply --var-file=./variables/<workspace_name>.tfvars
```

* To execute apply command for a particular module.

```
terraform apply --target=module.(module_name) --var-file=./variables/<workspace_name>.tfvars
```

eg:- terraform apply --target=module.ecs --var-file=./variables/dev.tfvars

#### Destroy


##### The **terraform destroy --var-file=./variables/<workspace>.tfvars** command is used to destroy the infrastructure. Make sure to select to appropriate workspace before destroying.

* To execute destroy command for all the modules.

```
terraform destroy --var-file=./variables/<workspace_name>.tfvars
```

* To execute destroy command for a particular module.

```
terraform destroy --target=module.(module_name) --var-file=./variables/<workspace_name>.tfvars
```

eg:- terraform destroy --target=module.ecs --var-file=./variables/dev.tfvars

**Note**: The destroy command on first phase shows an error with deletion of Lambda Edge functions. This is a known limitation of lambda edge since it is replicated across different edge locations. So in this case we need to wait for 10-15 minutes and then re-execute destroy command which deletes all resources.